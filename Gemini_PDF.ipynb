{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMIwp7AoOAdw",
        "outputId": "c5f36ae8-9325-4ca5-bb4e-f26f9acd2459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.7.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.6)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.19.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.6->google-generativeai) (1.24.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.63.2)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.20.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import google.generativeai as genai\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "def upload_to_gemini(path, mime_type=None):\n",
        "  \"\"\"Uploads the given file to Gemini.\n",
        "\n",
        "  See https://ai.google.dev/gemini-api/docs/prompting_with_media\n",
        "  \"\"\"\n",
        "  file = genai.upload_file(path, mime_type=mime_type)\n",
        "  print(f\"Uploaded file '{file.display_name}' as: {file.uri}\")\n",
        "  return file"
      ],
      "metadata": {
        "id": "QF-EXpcWOP33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wait_for_files_active(files):\n",
        "  \"\"\"Waits for the given files to be active.\n",
        "\n",
        "  Some files uploaded to the Gemini API need to be processed before they can be\n",
        "  used as prompt inputs. The status can be seen by querying the file's \"state\"\n",
        "  field.\n",
        "\n",
        "  This implementation uses a simple blocking polling loop. Production code\n",
        "  should probably employ a more sophisticated approach.\n",
        "  \"\"\"\n",
        "  print(\"Waiting for file processing...\")\n",
        "  for name in (file.name for file in files):\n",
        "    file = genai.get_file(name)\n",
        "    while file.state.name == \"PROCESSING\":\n",
        "      print(\".\", end=\"\", flush=True)\n",
        "      time.sleep(10)\n",
        "      file = genai.get_file(name)\n",
        "    if file.state.name != \"ACTIVE\":\n",
        "      raise Exception(f\"File {file.name} failed to process\")\n",
        "    return file\n",
        "  print(\"...all files ready\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "YC79krKbOkhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "generation_config = {\n",
        "  \"temperature\": 0,\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 64,\n",
        "  \"max_output_tokens\": 8192,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}"
      ],
      "metadata": {
        "id": "mGN6EiUGOm6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "  model_name=\"gemini-1.5-flash\",\n",
        "  generation_config=generation_config,\n",
        "  # safety_settings = Adjust safety settings\n",
        "  # See https://ai.google.dev/gemini-api/docs/safety-settings\n",
        ")\n",
        "\n",
        "# TODO Make these files available on the local file system\n",
        "# You may need to update the file paths\n",
        "files = [\n",
        "  upload_to_gemini(\"2407.01449v2.pdf\", mime_type=\"application/pdf\"),\n",
        "]\n",
        "\n",
        "# Some files have a processing delay. Wait for them to be ready.\n",
        "wait_for_files_active(files)\n",
        "\n",
        "chat_session = model.start_chat(\n",
        "  history=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        files[0],\n",
        "        \"What is the title of this paper?\\n\\n\",\n",
        "      ],\n",
        "    },\n",
        "\n",
        "  ]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Br086n8COFIf",
        "outputId": "287863ea-cfa4-4bd0-b157-fe714e28e504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded file '2407.01449v2.pdf' as: https://generativelanguage.googleapis.com/v1beta/files/p50159e4zcc8\n",
            "Waiting for file processing...\n",
            "...all files ready\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat_session.send_message(\"How many Figures are in the paper?\")\n",
        "\n",
        "print(response.text)\n",
        "\n",
        "print(response.usage_metadata)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "X8MkRlFYOFPK",
        "outputId": "c5a44de7-0f47-476e-c0cf-42ad06f41eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The paper contains 8 Figures. \n",
            "\n",
            "prompt_token_count: 30421\n",
            "candidates_token_count: 7\n",
            "total_token_count: 30428\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat_session.send_message(\"Who are the authors?\")\n",
        "\n",
        "print(response.text)\n",
        "print(response.usage_metadata)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "tPFiyd1cPGUh",
        "outputId": "082e782e-db7a-462d-b373-bf62d94ab3f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The authors of the paper are:\n",
            "\n",
            "* Manuel Faysse\n",
            "* Hugues Sibille\n",
            "* Tony Wu\n",
            "* Bilel Omrani\n",
            "* Gautier Viaud\n",
            "* Céline Hudelot\n",
            "* Pierre Colombo \n",
            "\n",
            "prompt_token_count: 30437\n",
            "candidates_token_count: 44\n",
            "total_token_count: 30481\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat_session.send_message(\"What are the major contributions of the paper accordig to the authors?\")\n",
        "\n",
        "print(response.text)\n",
        "\n",
        "print(response.usage_metadata)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "zG-iEcL4PI14",
        "outputId": "03d54295-eb04-41bd-839f-732286e6bbb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The authors highlight two major contributions of their paper:\n",
            "\n",
            "1. **ViDoRe:** A comprehensive benchmark for evaluating document retrieval systems that go beyond text-only systems and consider visual elements. \n",
            "2. **ColPali:** A novel document retrieval model architecture that leverages the capabilities of Vision Language Models (VLMs) to produce high-quality contextualized embeddings solely from images of document pages. \n",
            "\n",
            "The authors argue that ColPali outperforms modern document retrieval pipelines while being drastically faster and end-to-end trainable. \n",
            "\n",
            "prompt_token_count: 30499\n",
            "candidates_token_count: 109\n",
            "total_token_count: 30608\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8-XTWoEdPTeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Context Cache"
      ],
      "metadata": {
        "id": "YCpZJS6AH--B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = [\n",
        "  upload_to_gemini(\"2407.01449v2.pdf\", mime_type=\"application/pdf\"),\n",
        "]"
      ],
      "metadata": {
        "id": "GVMmD28mIA7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_pdf_file = '2403.06634.pdf'\n",
        "\n",
        "# Upload the video using the Files API\n",
        "pdf_file = genai.upload_file(path=path_to_pdf_file)\n",
        "\n",
        "# Wait for the file to finish processing\n",
        "while pdf_file.state.name == 'PROCESSING':\n",
        "  print('Waiting for video to be processed.')\n",
        "  time.sleep(2)\n",
        "  pdf_file = genai.get_file(pdf_file.name)\n",
        "\n",
        "print(f'Video processing complete: {pdf_file.uri}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6dyZZY1FIcLT",
        "outputId": "ac6a70b0-ec71-4254-eadf-677c2fbe2a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video processing complete: https://generativelanguage.googleapis.com/v1beta/files/a1lfs7l5tzwy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.generativeai import caching\n",
        "import datetime\n",
        "import time"
      ],
      "metadata": {
        "id": "Ym5nDUNFJPbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a cache with a 5 minute TTL\n",
        "cache = caching.CachedContent.create(\n",
        "    model='models/gemini-1.5-flash-001',\n",
        "    display_name='PDF-file', # used to identify the cache\n",
        "    system_instruction=(\n",
        "        'You are an expert PDF file analyzer, and your job is to answer '\n",
        "        'the user\\'s query based on the PDF file you have access to.'\n",
        "    ),\n",
        "    contents=[pdf_file],\n",
        "    ttl=datetime.timedelta(minutes=15),\n",
        ")\n"
      ],
      "metadata": {
        "id": "W1Qv5RqGJDqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a GenerativeModel which uses the created cache.\n",
        "model = genai.GenerativeModel.from_cached_content(cached_content=cache)\n",
        "\n",
        "# Query the model\n",
        "response = model.generate_content([(\n",
        "    'What is the title of the paper?'\n",
        "    'Who are the authors? '\n",
        "    'What are the major contributions of the paper accordig to the authors?'\n",
        "    'they were introduced for the first time.'\n",
        ")])\n",
        "\n",
        "print(response.usage_metadata)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "5Mfw0joBJQ7L",
        "outputId": "158fabda-ba5d-4917-9a0d-1e4cd338f318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt_token_count: 33300\n",
            "candidates_token_count: 261\n",
            "total_token_count: 33561\n",
            "cached_content_token_count: 33264\n",
            "\n",
            "The paper is titled \"Stealing Part of a Production Language Model\". \n",
            "\n",
            "The authors are:\n",
            "- Nicholas Carlini\n",
            "- Daniel Paleka\n",
            "- Krishnamurthy (Dj) Dvijotham\n",
            "- Thomas Steinke\n",
            "- Jonathan Hayase\n",
            "- A. Feder Cooper\n",
            "- Katherine Lee\n",
            "- Matthew Jagielski\n",
            "- Milad Nasr\n",
            "- Arthur Conmy\n",
            "- Eric Wallace\n",
            "- David Rolnick\n",
            "- Florian Tramèr\n",
            "\n",
            "The paper introduces the first model-stealing attack that can be applied to black-box language models. This attack allows the authors to recover the complete embedding projection layer of a transformer language model. \n",
            "\n",
            "The paper's major contributions are:\n",
            "- The attack is the first to extract precise, nontrivial information from black-box production language models.\n",
            "- The attack recovers the embedding projection layer of a transformer model given typical API access. \n",
            "- The attack confirms for the first time that OpenAI's Ada and Babbage language models have a hidden dimension of 1024 and 2048, respectively. \n",
            "- The attack recovers the exact hidden dimension size of the gpt-3.5-turbo model. \n",
            "- The authors discuss potential defenses and mitigations against the attack.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a GenerativeModel which uses the created cache.\n",
        "model = genai.GenerativeModel.from_cached_content(cached_content=cache)\n",
        "\n",
        "# Query the model\n",
        "response = model.generate_content([(\n",
        "    'What is the title of the paper?'\n",
        "    'Who are the authors? provide a list '\n",
        "    'What are the major contributions of the paper accordig to the authors?'\n",
        "    'they were introduced for the first time.'\n",
        ")])\n",
        "\n",
        "print(response.usage_metadata)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "5xVp4b6kMhXu",
        "outputId": "5d5bb376-fa18-49d4-e6aa-676fdc57c61a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt_token_count: 33303\n",
            "candidates_token_count: 247\n",
            "total_token_count: 33550\n",
            "cached_content_token_count: 33264\n",
            "\n",
            "The title of the paper is \"Stealing Part of a Production Language Model\".\n",
            "\n",
            "The authors of the paper are:\n",
            "\n",
            "1. Nicholas Carlini\n",
            "2. Daniel Paleka\n",
            "3. Krishnamurthy (Dj) Dvijotham\n",
            "4. Thomas Steinke\n",
            "5. Jonathan Hayase\n",
            "6. A. Feder Cooper\n",
            "7. Katherine Lee\n",
            "8. Matthew Jagielski\n",
            "9. Milad Nasr\n",
            "10. Arthur Conmy\n",
            "11. Eric Wallace\n",
            "12. David Rolnick\n",
            "13. Florian Tramèr\n",
            "\n",
            "The paper's major contributions are:\n",
            "\n",
            "- The first model-stealing attack that extracts precise, nontrivial information from black-box production language models, such as OpenAI's ChatGPT or Google's PaLM-2.\n",
            "- The attack recovers the embedding projection layer of a transformer model, given typical API access. \n",
            "- The authors confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively.\n",
            "- The authors discuss potential defenses and mitigations.\n",
            "- The authors discuss the implications of possible future work that could extend their attack.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for c in caching.CachedContent.list():\n",
        "  print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "q5Ec4WToKqGL",
        "outputId": "d4caea43-01d3-4dda-a147-d061321f5ac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CachedContent(\n",
            "    name='cachedContents/zqk8uilw4ek4',\n",
            "    model='models/gemini-1.5-flash-001',\n",
            "    display_name='PDF-file',\n",
            "    usage_metadata={\n",
            "        'total_token_count': 33264,\n",
            "    },\n",
            "    create_time=2024-08-11 06:52:05.138716+00:00,\n",
            "    update_time=2024-08-11 06:52:05.138716+00:00,\n",
            "    expire_time=2024-08-11 07:07:04.854054+00:00\n",
            ")\n",
            "CachedContent(\n",
            "    name='cachedContents/gmdhiut161iv',\n",
            "    model='models/gemini-1.5-flash-001',\n",
            "    display_name='sherlock jr movie',\n",
            "    usage_metadata={\n",
            "        'total_token_count': 33264,\n",
            "    },\n",
            "    create_time=2024-08-11 06:46:08.530272+00:00,\n",
            "    update_time=2024-08-11 06:46:08.530272+00:00,\n",
            "    expire_time=2024-08-11 07:01:07.812105+00:00\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"What is the main theme of the paper?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "gV_zK0l5NLOJ",
        "outputId": "09d5a149-787b-4f5f-b1c5-d53d6023ecb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The paper focuses on a model-stealing attack that extracts precise, non-trivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, the attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access.  The authors present a number of attacks, starting with an attack that only recovers the hidden dimension size of the model, then expanding to an attack that recovers the entire projection matrix. They also discuss potential defenses and mitigations for this type of attack.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.usage_metadata)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_ca2h1aPAXB",
        "outputId": "de4bd2df-1098-48dd-c022-e112a55ff2d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt_token_count: 33274\n",
            "candidates_token_count: 114\n",
            "total_token_count: 33388\n",
            "cached_content_token_count: 33264\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bs0j1L7wPS5_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}